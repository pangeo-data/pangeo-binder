binderhub:
  rbac:
    enabled: true

  config:
    BinderHub:
      build_image: jupyter/repo2docker:0.11.0-20.g070e4a9
      use_registry: true
      # use_named_servers: true  # turn back on to use auth
      # auth_enabled: true  # turn back on to use auth
      per_repo_quota: 150
      template_path: /etc/binderhub/custom/templates
      extra_static_path: /etc/binderhub/custom/static
      extra_static_url_prefix: /extra_static/
      about_message: |
        <p>binder.pangeo.io is public infrastructure operated by the <a href="https://pangeo.io">Pangeo Project team</a>.<br /><br />
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
    https:
      enabled: true
      type: kube-lego
  jupyterhub:
    ingress:
        enabled: true
        annotations:
          ingress.kubernetes.io/proxy-body-size: 64m
          kubernetes.io/ingress.class: nginx
          kubernetes.io/tls-acme: 'true'
    cull:
      # don't cull authenticated users
      # users: False  # turn back on to use auth
      # cull every 11 minutes so it is out of phase
      # with the proxy check-routes interval of five minutes
      every: 660
      timeout: 600
      # maxAge is 3 hours: 3 * 3600 = 10800
      maxAge: 10800
    scheduling:
      userScheduler:
        enabled: true
        replicas: 1
      podPriority:
        enabled: true
      userPlaceholder:
        enabled: false
      userPods:
        nodeAffinity:
          matchNodePurpose: require
    singleuser:
      # https://github.com/pangeo-data/pangeo-binder/issues/129
      serviceAccountName: pangeo
      cloudMetadata:
        enabled: true
      cpu:
        # Don't use same units as kubernetes resources here: TraitError: The 'cpu_limit' trait of a BinderSpawner instance must be a float, but a value of '4000m' <class 'str'> was specified.
        limit: 4
        guarantee: 1
      memory:
        # watch out for traitlets.traitlets.TraitError: 8Gi is not a valid memory specification. Must be an int or a string with suffix K, M, G, T
        limit: 8G
        guarantee: 8G
      cmd: jupyter-lab
      extraEnv:
        DASK_GATEWAY__AUTH__TYPE: "jupyterhub"
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'

      # to make notebook servers aware of hub
      # cmd: jupyterhub-singleuser  # turn back on to use auth
      # start in jupyter lab
      # defaultUrl: "/lab"  # turn back on to use auth
    hub:
      resources:
        requests:
          cpu: 250m
          memory: 512Mi
        limits:
          cpu: 1250m
          memory: 1Gi
      # redirectToServer: false  # turn back on to use auth
      # allowNamedServers: true  # turn back on to use auth
      # change this value as you wish,
      # or remove this line if you don't want to have any limit
      # namedServerLimitPerUser: 5  # turn back on to use auth
      # redirectToServer: false  # turn back on to use auth
      # services:  # turn back on to use auth
      #   binder:
      #     oauth_redirect_uri: "https://staging.binder.pangeo.io/hub/oauth_callback"
      #     oauth_client_id: "staging.binder.pangeo.io"
      # extraConfig:
      #   binder: |
      #     from kubespawner import KubeSpawner

      #     class BinderSpawner(KubeSpawner):
      #       def start(self):
      #           if 'image' in self.user_options:
      #             # binder service sets the image spec via user options
      #             self.image = self.user_options['image']
      #           return super().start()
      #     c.JupyterHub.spawner_class = BinderSpawner
    proxy:
      chp:
        resources:
          requests:
            cpu: 250m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi

      nginx:
        resources:
          requests:
            cpu: 250m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi


  initContainers:
    - name: git-clone-templates
      image: alpine/git
      args:
        - clone
        - --single-branch
        - --branch=master
        - --depth=1
        - --
        - https://github.com/pangeo-data/pangeo-custom-binderhub-templates.git
        - /etc/binderhub/custom
      securityContext:
        runAsUser: 0
      volumeMounts:
        - name: custom-templates
          mountPath: /etc/binderhub/custom
  extraVolumes:
    - name: custom-templates
      emptyDir: {}
  extraVolumeMounts:
    - name: custom-templates
      mountPath: /etc/binderhub/custom

  dind:
    enabled: true
    daemonset:
      image:
        name: docker
        tag: 19.03.5-dind
  imageCleaner:
    enabled: true
    # when 80% of inodes are used,
    # cull images until only 40% are used.
    imageGCThresholdHigh: 80
    imageGCThresholdLow: 40
    host:
      enabled: false

nginx-ingress:
  rbac:
    create: true
  defaultBackend:
    minAvailable: 0

  controller:
    replicaCount: 1
    scope:
      enabled: true
    config:
      # Allow POSTs of upto 64MB, for large notebook support.
      proxy-body-size: 64m
    resources:
      requests:
        cpu: 250m
        memory: 240Mi
      limits:
        cpu: 500m
        memory: 240Mi

    service:
      # Preserve client IPs
      externalTrafficPolicy: Local

kube-lego:
  config:
    LEGO_EMAIL: jhamman@ucar.edu
    LEGO_URL: https://acme-v01.api.letsencrypt.org/directory
  rbac:
    create: true
  image:
    tag: 0.1.7

dask-gateway:
  gateway:
    prefix: "/services/dask-gateway"
    image:
      name: daskgateway/dask-gateway-server
      tag: 0.7.1
      pullPolicy: IfNotPresent

    backend:
      scheduler:
        cores:
          request: 0.8
          limit: 1
        memory:
          request: 1G
          limit: 2G
        extraPodConfig:
#          name: scheduler-{JUPYTERHUB_USER}-{uuid}
          tolerations:
            - key: "k8s.dask.org/dedicated"
              operator: "Equal"
              value: "scheduler"
              effect: "NoSchedule"
            - key: "k8s.dask.org_dedicated"
              operator: "Equal"
              value: "scheduler"
              effect: "NoSchedule"

      worker:
        extraContainerConfig:
          securityContext:
            runAsGroup: 1000
            runAsUser: 1000
        extraPodConfig:
#          name: worker-{JUPYTERHUB_USER}-{uuid}
          serviceAccountName: pangeo
          automountServiceAccountToken: true
          securityContext:
            fsGroup: 1000
          tolerations:
            - key: "k8s.dask.org/dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "k8s.dask.org_dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"

    extraConfig:
      # Use the mapping form, to support merging multiple values.yaml
      optionHandler: |
        from dask_gateway_server.options import Options, Integer, Float, String

        def cluster_options(user):
           def option_handler(options):
               if ":" not in options.image:
                   raise ValueError("When specifying an image you must also provide a tag")
               extra_labels = {
                   "jupyterhub/username": user.name
               }
               return {
                   "worker_cores_limit": options.worker_cores,
                   "worker_cores": min(options.worker_cores / 2, 1),
                   "worker_memory": "%fG" % options.worker_memory,
                   "image": options.image,
                   "scheduler_extra_pod_labels": extra_labels,
                   "worker_extra_pod_labels": extra_labels,
               }
           return Options(
               Integer("worker_cores", 2, min=1, max=4, label="Worker Cores"),
               Float("worker_memory", 4, min=1, max=8, label="Worker Memory (GiB)"),
               String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
               handler=option_handler,
           )
        
        c.Backend.cluster_options = cluster_options
      # TODO: replace with some k8s things
      # resource quota. per namespace.
      # on binder, so might need some thought.
      # maybe max time for now.
      # userLimts: |
      #   c.UserLimits.max_cores = 100
      #   c.UserLimits.max_memory = "400 G"
      #   c.UserLimits.max_clusters = 1
